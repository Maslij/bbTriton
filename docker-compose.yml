version: '2.3'

services:
  triton-server:
    image: 246261010633.dkr.ecr.ap-southeast-2.amazonaws.com/bbtriton-jetson:latest
    container_name: triton-server-jetson
    restart: unless-stopped
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - MAX_BATCH_SIZE=${MAX_BATCH_SIZE:-4}
      - OPT_BATCH_SIZE=${OPT_BATCH_SIZE:-2}
      - FORCE_BUILD=${FORCE_BUILD:-false}
    ports:
      - "8000:8000"  # HTTP endpoint
      - "8001:8001"  # gRPC endpoint
      - "8002:8002"  # Metrics endpoint
    volumes:
      - ./models:/apps/models  # Persist built models
      - ./models_onnx:/apps/models_onnx  # Persist downloaded ONNX models
      - /dev/shm:/dev/shm
    working_dir: /apps
    shm_size: 2g
    ulimits:
      memlock: -1
      stack: 67108864
    ipc: host
    # No need for entrypoint specification as it's defined in the Dockerfile 